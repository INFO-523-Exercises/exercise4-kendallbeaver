---
title: "exercise4"
author: "Kendall Beaver"
format: html
editor: visual
---

```{r}
#################
# Load packages
#################

install.packages("ggplot2")
install.packages("DescTools")
install.packages("gridExtra")

library(ggplot2)
library(tibble)
library(rsample)
library(parsnip)
library(dplyr)
library(Metrics)
library(caret)
library(DescTools)
library(gridExtra)
```

```{r}
##################################
#
# Generate Synthetic Attributes
#
##################################

# parameters
seed <- 1            # seed for random number generation 
numInstances <- 200  # number of data instances

# Set seed
set.seed(seed)

# Generate data
X <- matrix(runif(numInstances), ncol=1)
y_true <- -3*X + 1 
y <- y_true + matrix(rnorm(numInstances), ncol=1)

# Plot
ggplot() +
  geom_point(aes(x=X, y=y), color="black") +
  geom_line(aes(x=X, y=y_true), color="blue", linewidth=1) +
  ggtitle('True function: y = -3X + 1') +
  xlab('X') +
  ylab('y')

```

```{r}
#######################################################
# Step 1: Split Input Data into Training and Test Sets
#######################################################

# Train/test split
numTrain <- 20   # number of training instances
numTest <- numInstances - numTrain

set.seed(123) # For reproducibility

data <- tibble(X = X, y = y)

split_obj <- initial_split(data, prop = numTrain/numInstances)

# Extract train and test data
train_data <- training(split_obj)
test_data <- testing(split_obj)

# Extract X_train, X_test, y_train, y_test
X_train <- train_data$X
y_train <- train_data$y

X_test <- test_data$X
y_test <- test_data$y
```

```{r}

######################################################
# Step 2: Fit/Create Regression Model to Training Set
######################################################

# Create a linear regression model specification
lin_reg_spec <- linear_reg() |> # This just contains info that says "lm".
  set_engine("lm")

# Fit the model to the training data
lin_reg_fit <- lin_reg_spec |> ### So this is OUR training model.
  fit(y ~ X, data = train_data)
```

```{r}

################################################
# Step 3: Apply Model to the Test Set
################################################

# Apply model to the test set
y_pred_test <- predict(lin_reg_fit, new_data = test_data) |>
  pull(.pred) # This generates 180 predicted target/y-values.
```

```{r}

###########################################################
# Step 4: Evaluate Model Performance on Test Set (Visually)
###########################################################

# Plotting true vs predicted values
ggplot() + 
  geom_point(aes(x = as.vector(y_test), y = y_pred_test), color = 'black') +
  ggtitle('Comparing true and predicted values for test set') +
  xlab('True values for y') +
  ylab('Predicted values for y')
```

```{r}
#####################
# Step 4 (CONT'D)
#####################

# Prepare data for yardstick evaluation, "truth estimates"
eval_data <- tibble(
  truth = as.vector(y_test),
  estimate = y_pred_test
)

truth <- as.vector(y_test)
estimate <- y_pred_test

# Model evaluation

# rmse_value <- rmse(data = eval_data, truth = truth, estimate = estimate)
# The above function was Greg's function and it wasn't working
rmse_value <- rmse(truth, estimate)
rmse_value <- as.data.frame(rmse_value)
rmse_value

# r2_value <- rsq(eval_data, truth = truth, estimate = estimate)
# The above function wasn't working 
r2_value <- R2(estimate, truth)

cat("Root mean squared error =", sprintf("%.4f", rmse_value), "\n")
# Greg's original function that didn't work: cat("Root mean squared error =", sprintf("%.4f", rmse_value$.estimate), "\n")

cat('R-squared =', sprintf("%.4f", r2_value), "\n")
# Greg's original function that didn't work: cat('R-squared =', sprintf("%.4f", r2_value$.estimate), "\n")
```

```{r}

#########################
# Step 5: Postprocessing
#########################

# Display model parameters
coef_values <- coef(lin_reg_fit$fit)  # Extract coefficients
slope <- coef_values["X"]
intercept <- coef_values["(Intercept)"]

cat("Slope =", slope, "\n")

cat("Intercept =", intercept, "\n")

### Step 4: Postprocessing

# Plot outputs
ggplot() +
  geom_point(aes(x = as.vector(X_test), y = as.vector(y_test)), color = 'black') +
  geom_line(aes(x = as.vector(X_test), y = y_pred_test), color = 'blue', linewidth = 1) +
  ggtitle(sprintf('Predicted Function: y = %.2fX + %.2f', slope, intercept)) +
  xlab('X') +
  ylab('y')

# Original/true function: -3*X + 1
```

```{r}

##################################
# Effect of Correlated Attributes
##################################

# Generate the variables, creating X2-X5.
set.seed(1)
X2 <- 0.5 * X + rnorm(numInstances, mean=0, sd=0.04)
X3 <- 0.5 * X2 + rnorm(numInstances, mean=0, sd=0.01)
X4 <- 0.5 * X3 + rnorm(numInstances, mean=0, sd=0.01)
X5 <- 0.5 * X4 + rnorm(numInstances, mean=0, sd=0.01)

# Create plots
plot1 <- ggplot() +
  geom_point(aes(X, X2), color='black') +
  xlab('X') + ylab('X2') +
  ggtitle(sprintf("Correlation between X and X2 = %.4f", cor(X[-c((numInstances-numTest+1):numInstances)], X2[-c((numInstances-numTest+1):numInstances)])))

plot2 <- ggplot() +
  geom_point(aes(X2, X3), color='black') +
  xlab('X2') + ylab('X3') +
  ggtitle(sprintf("Correlation between X2 and X3 = %.4f", cor(X2[-c((numInstances-numTest+1):numInstances)], X3[-c((numInstances-numTest+1):numInstances)])))

plot3 <- ggplot() +
  geom_point(aes(X3, X4), color='black') +
  xlab('X3') + ylab('X4') +
  ggtitle(sprintf("Correlation between X3 and X4 = %.4f", cor(X3[-c((numInstances-numTest+1):numInstances)], X4[-c((numInstances-numTest+1):numInstances)])))

plot4 <- ggplot() +
  geom_point(aes(X4, X5), color='black') +
  xlab('X4') + ylab('X5') +
  ggtitle(sprintf("Correlation between X4 and X5 = %.4f", cor(X4[-c((numInstances-numTest+1):numInstances)], X5[-c((numInstances-numTest+1):numInstances)])))

# Combine plots into a 2x2 grid/as one entire picture
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)
```

```{r}

# Split data into training and testing sets
train_indices <- 1:(numInstances - numTest) #200 - 180
test_indices <- (numInstances - numTest + 1):numInstances

# Create combined training and testing sets
X_train2 <- cbind(X[train_indices], X2[train_indices])
X_test2 <- cbind(X[test_indices], X2[test_indices])

X_train3 <- cbind(X[train_indices], X2[train_indices], X3[train_indices])
X_test3 <- cbind(X[test_indices], X2[test_indices], X3[test_indices])

X_train4 <- cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices])
X_test4 <- cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices])

X_train5 <- cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices], X5[train_indices])
X_test5 <- cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices], X5[test_indices])
```
